# LoRA adapter configuration
r: 32                      # Rank (low-rank dimension)
lora_alpha: 32             # Scaling factor, often rank or rank*2
lora_dropout: 0.0          # Dropout probability (0.0 is fastest/most memory-efficient)
bias: "none"               # "none", "all", or "lora_only"
task_type: "CAUSAL_LM"     # Required for causal LM fine-tuning

target_modules:            # Inject LoRA into these layers
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

use_rslora: false          # Rank-stabilized LoRA toggle
# loftq_config:
#   quant_type: "bnb_4bit"   # example â€” use values that match your PEFT version
#   weight_dtype: "float16"
